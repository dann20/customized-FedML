{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "weekly-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from shutil import unpack_archive\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "another-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = dict()\n",
    "urls['ecg']=['http://www.cs.ucr.edu/~eamonn/discords/ECG_data.zip',\n",
    "             'http://www.cs.ucr.edu/~eamonn/discords/mitdbx_mitdbx_108.txt',\n",
    "             'http://www.cs.ucr.edu/~eamonn/discords/qtdbsele0606.txt',\n",
    "             'http://www.cs.ucr.edu/~eamonn/discords/chfdbchf15.txt',\n",
    "             'http://www.cs.ucr.edu/~eamonn/discords/qtdbsel102.txt']\n",
    "urls['gesture']=['http://www.cs.ucr.edu/~eamonn/discords/ann_gun_CentroidA']\n",
    "urls['space_shuttle']=['http://www.cs.ucr.edu/~eamonn/discords/TEK16.txt',\n",
    "                       'http://www.cs.ucr.edu/~eamonn/discords/TEK17.txt',\n",
    "                       'http://www.cs.ucr.edu/~eamonn/discords/TEK14.txt']\n",
    "urls['respiration']=['http://www.cs.ucr.edu/~eamonn/discords/nprs44.txt',\n",
    "                     'http://www.cs.ucr.edu/~eamonn/discords/nprs43.txt']\n",
    "urls['power_demand']=['http://www.cs.ucr.edu/~eamonn/discords/power_data.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fitted-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(urls):\n",
    "    for dataname in urls:\n",
    "        raw_dir = Path('datasets', dataname, 'raw')\n",
    "        raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "        for url in urls[dataname]:\n",
    "            filename = raw_dir.joinpath(Path(url).name)\n",
    "            print('Downloading', url)\n",
    "            resp =requests.get(url)\n",
    "            filename.write_bytes(resp.content)\n",
    "            if filename.suffix=='':\n",
    "                filename.rename(filename.with_suffix('.txt'))\n",
    "            print('Saving to', filename.with_suffix('.txt'))\n",
    "            if filename.suffix=='.zip':\n",
    "                print('Extracting to', filename)\n",
    "                unpack_archive(str(filename), extract_dir=str(raw_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "secure-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_dataset(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "electoral-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function load one .cvs (a sequence)\n",
    "def load_data(dataset, dataset_folder='datasets'):\n",
    "    raw_dir = Path('datasets', dataset, 'raw')\n",
    "    readings = {}\n",
    "    idx_anomaly = {}\n",
    "    for filepath in raw_dir.glob('*.txt'):\n",
    "        with open(str(filepath)) as f:\n",
    "            for i, line in enumerate(f):\n",
    "                tokens = [float(token) for token in line.split()]\n",
    "                if raw_dir.parent.name == 'ecg':\n",
    "                    tokens.pop(0)\n",
    "                if filepath.name == 'chfdbchf15.txt':\n",
    "                    tokens.append(1.0) if 2250 < i < 2400 else tokens.append(0.0)\n",
    "                elif filepath.name == 'xmitdb_x108_0.txt':\n",
    "                    tokens.append(1.0) if 4020 < i < 4400 else tokens.append(0.0)\n",
    "                elif filepath.name == 'mitdb__100_180.txt':\n",
    "                    tokens.append(1.0) if 1800 < i < 1990 else tokens.append(0.0)\n",
    "                elif filepath.name == 'chfdb_chf01_275.txt':\n",
    "                    tokens.append(1.0) if 2330 < i < 2600 else tokens.append(0.0)\n",
    "                elif filepath.name == 'ltstdb_20221_43.txt':\n",
    "                    tokens.append(1.0) if 650 < i < 780 else tokens.append(0.0)\n",
    "                elif filepath.name == 'ltstdb_20321_240.txt':\n",
    "                    tokens.append(1.0) if 710 < i < 850 else tokens.append(0.0)\n",
    "                elif filepath.name == 'chfdb_chf13_45590.txt':\n",
    "                    tokens.append(1.0) if 2800 < i < 2960 else tokens.append(0.0)\n",
    "                elif filepath.name == 'stdb_308_0.txt':\n",
    "                    tokens.append(1.0) if 2290 < i < 2550 else tokens.append(0.0)\n",
    "                elif filepath.name == 'qtdbsel102.txt':\n",
    "                    tokens.append(1.0) if 4230 < i < 4430 else tokens.append(0.0)\n",
    "                elif filepath.name == 'ann_gun_CentroidA.txt':\n",
    "                    tokens.append(1.0) if 2070 < i < 2810 else tokens.append(0.0)\n",
    "                elif filepath.name == 'TEK16.txt':\n",
    "                    tokens.append(1.0) if 4270 < i < 4370 else tokens.append(0.0)\n",
    "                elif filepath.name == 'TEK17.txt':\n",
    "                    tokens.append(1.0) if 2100 < i < 2145 else tokens.append(0.0)\n",
    "                elif filepath.name == 'TEK14.txt':\n",
    "                    tokens.append(1.0) if 1100 < i < 1200 or 1455 < i < 1955 else tokens.append(0.0)\n",
    "                elif filepath.name == 'nprs44.txt':\n",
    "                    tokens.append(1.0) if 16192 < i < 16638 or 20457 < i < 20911 else tokens.append(0.0)\n",
    "                elif filepath.name == 'nprs43.txt':\n",
    "                    tokens.append(1.0) if 12929 < i < 13432 or 14877 < i < 15086 or 15729 < i < 15924 else tokens.append(0.0)\n",
    "                elif filepath.name == 'power_data.txt':\n",
    "                    tokens.append(1.0) if 8254 < i < 8998 or 11348 < i < 12143 or 33883 < i < 34601 else tokens.append(0.0)\n",
    "                try:\n",
    "                    readings[filepath.name].append(tokens[:-1])\n",
    "                    if tokens[-1] == 1.0:\n",
    "                        idx_anomaly[filepath.name].append(i)\n",
    "                except:\n",
    "                    readings[filepath.name] = [tokens[:-1]]\n",
    "                    if tokens[-1] == 1.0:\n",
    "                        idx_anomaly[filepath.name] = [i]\n",
    "        if dataset == 'ecg':\n",
    "            readings[filepath.name] = readings[filepath.name][1:]\n",
    "        readings[filepath.name] = np.asarray(readings[filepath.name])\n",
    "        # print(filepath.name, readings[filepath.name].shape)\n",
    "        idx_anomaly[filepath.name] = np.asarray(idx_anomaly[filepath.name])\n",
    "        print(readings[filepath.name].shape)\n",
    "    return idx_anomaly, readings\n",
    "#     if dataset == 'ambient_temp':\n",
    "#         data_file = os.path.join(csv_folder, 'ambient_temperature_system_failure.csv')\n",
    "#         anomalies = ['2013-12-22 20:00:00', '2014-04-13 09:00:00']\n",
    "#         t_unit = 'hour'\n",
    "#     elif dataset == 'cpu_utilization':\n",
    "#         data_file = os.path.join(csv_folder, 'cpu_utilization_asg_misconfiguration.csv')\n",
    "#         anomalies = ['2014-07-12 02:04:00', '2014-07-14 21:44:00']\n",
    "#         t_unit = '5 min'\n",
    "#     elif dataset == 'ec2_request':\n",
    "#         data_file = os.path.join(csv_folder, 'ec2_request_latency_system_failure.csv')\n",
    "#         anomalies = ['2014-03-14 09:06:00', '2014-03-18 22:41:00', '2014-03-21 03:01:00']\n",
    "#         t_unit = '5 min'\n",
    "#     elif dataset == 'machine_temp':\n",
    "#         data_file = os.path.join(csv_folder, 'machine_temperature_system_failure.csv')\n",
    "#         anomalies = ['2013-12-11 06:00:00', '2013-12-16 17:25:00', '2014-01-28 13:55:00', '2014-02-08 14:30:00']\n",
    "#         t_unit = '5 min'\n",
    "#     elif dataset == 'rogue_agent_key_hold':\n",
    "#         data_file = os.path.join(csv_folder, 'rogue_agent_key_hold.csv')\n",
    "#         anomalies = ['2014-07-15 08:30:00', '2014-07-17 09:50:00']\n",
    "#         t_unit = '5 min'\n",
    "#     elif dataset == 'rogue_agent_key_updown':\n",
    "#         data_file = os.path.join(csv_folder, 'rogue_agent_key_updown.csv')\n",
    "#         anomalies = ['2014-07-15 04:00:00', '2014-07-17 08:50:00']\n",
    "#         t_unit = '5 min'\n",
    "#     elif dataset == 'nyc_taxi':\n",
    "#         data_file = os.path.join(csv_folder, 'nyc_taxi.csv')\n",
    "#         anomalies = ['2014-11-01 19:00:00', '2014-11-27 15:30:00', '2014-12-25 15:00:00', '2015-01-01 01:00:00', \n",
    "#                      '2015-01-27 00:00:00']\n",
    "#         t_unit = '30 min'\n",
    "    \n",
    "#     t = []\n",
    "#     readings = []\n",
    "#     idx_anomaly = []\n",
    "#     i = 0\n",
    "#     with open(data_file) as csvfile:\n",
    "#         readCSV = csv.reader(csvfile, delimiter=',')\n",
    "#         print(\"\\n--> Anomalies occur at:\")\n",
    "#         for row in readCSV:\n",
    "#             if i > 0:\n",
    "#                 t.append(i)\n",
    "#                 readings.append(float(row[1]))\n",
    "#                 for j in range(len(anomalies)):\n",
    "#                     if row[0] == anomalies[j]:\n",
    "#                         idx_anomaly.append(i)\n",
    "#                         print(\"  timestamp #{}: {}\".format(j, row[0]))\n",
    "#             i = i + 1\n",
    "#     t = np.asarray(t)\n",
    "#     readings = np.asarray(readings)\n",
    "#     print(\"\\nOriginal csv file contains {} timestamps.\".format(t.shape))\n",
    "#     print(\"Processed time series contain {} readings.\".format(readings.shape))\n",
    "#     print(\"Anomaly indices are {}\".format(idx_anomaly))\n",
    "    \n",
    "#     return t, t_unit, readings, idx_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fitting-cedar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26785, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'power_data.txt': array([ 8255,  8256,  8257, ..., 34598, 34599, 34600])},\n",
       " {'power_data.txt': array([[884.],\n",
       "         [879.],\n",
       "         [846.],\n",
       "         ...,\n",
       "         [627.],\n",
       "         [633.],\n",
       "         [882.]])})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data('power_demand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "operating-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function plots a dataset with the train/test split and known anomalies\n",
    "# Relies on helper function load_data()\n",
    "\n",
    "def process_and_save_specified_dataset(dataset, y_scale=5, save_file=False):\n",
    "    all_idx_anomaly, all_readings = load_data(dataset)\n",
    "    all_readings_normalised = {}\n",
    "    for key in all_readings.keys():\n",
    "        readings = all_readings[key]\n",
    "        idx_anomaly = all_idx_anomaly[key]\n",
    "        # split into training and test sets\n",
    "\n",
    "        idx_train = idx_test = []\n",
    "        if key == 'chfdb_chf13_45590.txt':\n",
    "            idx_train = [0, 2439]\n",
    "            idx_test = [2439, 3726]\n",
    "        elif key == 'chfdb_chf01_275.txt':\n",
    "            idx_train = [0, 1833]\n",
    "            idx_test = [1833, 3674]\n",
    "        elif key == 'chfdbchf15.txt':\n",
    "            idx_train = [3381, 14244]\n",
    "            idx_test = [33, 3381]\n",
    "        elif key == 'qtdbsel102.txt':\n",
    "            idx_train = [10093, 44828]\n",
    "            idx_test = [211, 10093]\n",
    "        elif key == 'mitdb__100_180.txt':\n",
    "            idx_train = [2328, 5271]\n",
    "            idx_test = [73, 2328]\n",
    "        elif key == 'stdb_308_0.txt':\n",
    "            idx_train = [2986, 5359]\n",
    "            idx_test = [265, 2986]\n",
    "        elif key == 'ltstdb_20321_240.txt':\n",
    "            idx_train = [1520, 3531]\n",
    "            idx_test = [73, 1520]\n",
    "        elif key == 'xmitdb_x108_0.txt':\n",
    "            idx_train = [424, 3576]\n",
    "            idx_test = [3576, 5332]\n",
    "        elif key == 'ltstdb_20221_43.txt':\n",
    "            idx_train = [1121, 3731]\n",
    "            idx_test = [0, 1121]\n",
    "        elif key == 'ann_gun_CentroidA.txt':\n",
    "            idx_train = [3000, len(readings)]\n",
    "            idx_test = [0, 3000]\n",
    "        elif key == 'nprs44.txt':\n",
    "            idx_train = [363, 12955]\n",
    "            idx_test = [12955, 24082]\n",
    "        elif key == 'nprs43.txt':\n",
    "            idx_train = [4285, 10498]\n",
    "            idx_test = [10498, 17909]\n",
    "        elif key == 'power_data.txt':\n",
    "            idx_train = [15287, 33432]\n",
    "            idx_test = [501, 15287]\n",
    "        elif key == 'TEK17.txt':\n",
    "            idx_train = [2469, 4588]\n",
    "            idx_test = [1543, 2469]\n",
    "        elif key == 'TEK16.txt':\n",
    "            idx_train = [521, 3588]\n",
    "            idx_test = [3588, 4539]\n",
    "        elif key == 'TEK14.txt':\n",
    "            idx_train = [2089, 4098]\n",
    "            idx_test = [97, 2089]\n",
    "        training = readings[idx_train[0]:idx_train[1]]\n",
    "        # normalise by training mean and std \n",
    "        train_m = np.mean(training, axis=0)\n",
    "        train_std = np.std(training, axis=0)\n",
    "        readings_normalised = (readings - train_m) / train_std\n",
    "\n",
    "        training = readings_normalised[idx_train[0]:idx_train[1]]\n",
    "        test = readings_normalised[idx_test[0]:idx_test[1]]\n",
    "        idx_anomaly_test = idx_anomaly - idx_test[0]\n",
    "\n",
    "        if save_file:\n",
    "            save_dir = './datasets/{}/'.format(dataset) \n",
    "            np.savez(save_dir+key.replace('txt', 'npz'), readings=readings, idx_anomaly=idx_anomaly,\n",
    "                        training=training, test=test, train_m=train_m, train_std=train_std,\n",
    "                        idx_anomaly_test=idx_anomaly_test)\n",
    "            print(\"\\nProcessed time series are saved at {}\".format(save_dir+key.replace('txt', 'npz')))\n",
    "        else:\n",
    "            print(\"\\nProcessed time series are not saved.\")\n",
    "        all_readings_normalised[key] = readings_normalised\n",
    "\n",
    "#         # plot the whole normalised sequence\n",
    "#         fig, axs = plt.subplots(1, 1, figsize=(18, 4), edgecolor='k')\n",
    "#         fig.subplots_adjust(hspace=.4, wspace=.4)\n",
    "#         # axs = axs.ravel()\n",
    "#         # for i in range(4):\n",
    "#         axs.plot(t, readings_normalised)\n",
    "#         if idx_split[0] == 0:\n",
    "#             axs.plot(idx_split[1]*np.ones(20), np.linspace(-y_scale,y_scale,20), 'b--')\n",
    "#         else:\n",
    "#             for i in range(2):\n",
    "#                 axs.plot(idx_split[i]*np.ones(20), np.linspace(-y_scale,y_scale,20), 'b--')\n",
    "#         for j in range(len(idx_anomaly)):\n",
    "#             axs.plot(idx_anomaly[j]*np.ones(20), np.linspace(-y_scale,y_scale,20), 'r--')\n",
    "#         #     axs.plot(data[:,1])\n",
    "#         axs.grid(True)\n",
    "#         axs.set_xlim(0, len(t))\n",
    "#         axs.set_ylim(-y_scale, y_scale)\n",
    "#         axs.set_xlabel(\"timestamp (every {})\".format(t_unit))\n",
    "#         axs.set_ylabel(\"normalised readings\")\n",
    "#         axs.set_title(\"{} dataset\\n(normalised by train mean {:.2f} and std {:.2f})\".format(dataset, train_m, train_std))\n",
    "#         axs.legend(('data', 'train test set split', 'anomalies'))\n",
    "\n",
    "    return all_readings_normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "enhanced-antarctica",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9180, 2)\n",
      "\n",
      "Processed time series are saved at ./datasets/gesture/ann_gun_CentroidA.npz\n",
      "(729, 1)\n",
      "(3899, 1)\n",
      "(2899, 1)\n",
      "\n",
      "Processed time series are saved at ./datasets/space_shuttle/TEK16.npz\n",
      "\n",
      "Processed time series are saved at ./datasets/space_shuttle/TEK14.npz\n",
      "\n",
      "Processed time series are saved at ./datasets/space_shuttle/TEK17.npz\n",
      "(5122, 1)\n",
      "(7932, 1)\n",
      "\n",
      "Processed time series are saved at ./datasets/respiration/nprs43.npz\n",
      "\n",
      "Processed time series are saved at ./datasets/respiration/nprs44.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'nprs43.txt': array([[ 0.08671985],\n",
       "        [-0.03580043],\n",
       "        [-0.14081781],\n",
       "        ...,\n",
       "        [-8.51595371],\n",
       "        [-8.79600005],\n",
       "        [-7.65831179]]),\n",
       " 'nprs44.txt': array([[-0.66196   ],\n",
       "        [-0.66196   ],\n",
       "        [-0.66196   ],\n",
       "        ...,\n",
       "        [-2.41917013],\n",
       "        [-2.4654125 ],\n",
       "        [-2.36581355]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process_and_save_specified_dataset('ecg', save_file=True)\n",
    "# process_and_save_specified_dataset('power_demand', save_file=True)\n",
    "process_and_save_specified_dataset('gesture', save_file=True)\n",
    "process_and_save_specified_dataset('space_shuttle', save_file=True)\n",
    "process_and_save_specified_dataset('respiration', save_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-carpet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
